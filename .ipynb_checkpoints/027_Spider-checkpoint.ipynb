{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b576f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: \n",
      "['http://www.dr-chuck.com']\n",
      "How many pages:100\n",
      "Row : (1, 'http://www.dr-chuck.com')\n",
      "1 http://www.dr-chuck.com (6449) 3\n",
      "Row : (3, 'http://www.dr-chuck.com/office')\n",
      "3 http://www.dr-chuck.com/office (7473) 0\n",
      "Row : (2, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')\n",
      "2 http://www.dr-chuck.com/dr-chuck/resume/index.htm (1889) 11\n",
      "Row : (13, 'http://www.dr-chuck.com/dr-chuck/resume')\n",
      "13 http://www.dr-chuck.com/dr-chuck/resume (1889) 11\n",
      "Row : (6, 'http://www.dr-chuck.com/dr-chuck/resume/resume2018-08.doc')\n",
      "6 http://www.dr-chuck.com/dr-chuck/resume/resume2018-08.doc Ignore non text/html page\n",
      "Row : (19, 'http://www.dr-chuck.com/dr-chuck/leadership.htm')\n",
      "19 http://www.dr-chuck.com/dr-chuck/leadership.htm Unable to retrieve or parse page\n",
      "Row : (12, 'http://www.dr-chuck.com/dr-chuck/resume/pictures/index.htm')\n",
      "12 http://www.dr-chuck.com/dr-chuck/resume/pictures/index.htm (926) 0\n",
      "Row : (21, 'http://www.dr-chuck.com/dr-chuck/pictures/index.htm')\n",
      "21 http://www.dr-chuck.com/dr-chuck/pictures/index.htm Unable to retrieve or parse page\n",
      "Row : (4, 'http://www.dr-chuck.com/sakai-book')\n",
      "4 http://www.dr-chuck.com/sakai-book (5843) 4\n",
      "Row : (8, 'http://www.dr-chuck.com/dr-chuck/resume/research2017-05.docx')\n",
      "8 http://www.dr-chuck.com/dr-chuck/resume/research2017-05.docx Ignore non text/html page\n",
      "Row : (25, 'http://www.dr-chuck.com/errata.txt')\n",
      "25 http://www.dr-chuck.com/errata.txt Unable to retrieve or parse page\n",
      "Row : (10, 'http://www.dr-chuck.com/dr-chuck/resume/leadership.htm')\n",
      "10 http://www.dr-chuck.com/dr-chuck/resume/leadership.htm (6203) 1\n",
      "Row : (24, 'http://www.dr-chuck.com/html')\n",
      "24 http://www.dr-chuck.com/html Unable to retrieve or parse page\n",
      "Row : (15, 'http://www.dr-chuck.com/dr-chuck/resume2018-08.doc')\n",
      "15 http://www.dr-chuck.com/dr-chuck/resume2018-08.doc Unable to retrieve or parse page\n",
      "Row : (7, 'http://www.dr-chuck.com/dr-chuck/resume/cv2023-05.doc')\n",
      "7 http://www.dr-chuck.com/dr-chuck/resume/cv2023-05.doc Ignore non text/html page\n",
      "Row : (23, 'http://www.dr-chuck.com/Sakai_ Building an Open Source Community - Charles R. Severance.PDF')\n",
      "23 http://www.dr-chuck.com/Sakai_ Building an Open Source Community - Charles R. Severance.PDF Unable to retrieve or parse page\n",
      "Row : (11, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')\n",
      "11 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm (17477) 2\n",
      "Row : (20, 'http://www.dr-chuck.com/dr-chuck/speaking.htm')\n",
      "20 http://www.dr-chuck.com/dr-chuck/speaking.htm Unable to retrieve or parse page\n",
      "Row : (17, 'http://www.dr-chuck.com/dr-chuck/research2017-05.docx')\n",
      "17 http://www.dr-chuck.com/dr-chuck/research2017-05.docx Unable to retrieve or parse page\n",
      "Row : (9, 'http://www.dr-chuck.com/dr-chuck/resume/teach2017-08.docx')\n",
      "9 http://www.dr-chuck.com/dr-chuck/resume/teach2017-08.docx Ignore non text/html page\n",
      "Row : (18, 'http://www.dr-chuck.com/dr-chuck/teach2017-08.docx')\n",
      "18 http://www.dr-chuck.com/dr-chuck/teach2017-08.docx Unable to retrieve or parse page\n",
      "Row : (16, 'http://www.dr-chuck.com/dr-chuck/cv2023-05.doc')\n",
      "16 http://www.dr-chuck.com/dr-chuck/cv2023-05.doc Unable to retrieve or parse page\n",
      "Row : (22, 'http://www.dr-chuck.com/Sakai_ Building an Open Source Community - Charles R. Severance.epub')\n",
      "22 http://www.dr-chuck.com/Sakai_ Building an Open Source Community - Charles R. Severance.epub Unable to retrieve or parse page\n",
      "Row : (14, 'http://www.dr-chuck.com/dr-chuck/bio.htm')\n",
      "14 http://www.dr-chuck.com/dr-chuck/bio.htm Unable to retrieve or parse page\n",
      "Row : (5, 'http://www.dr-chuck.com/dr-chuck/resume/bio.htm')\n",
      "5 http://www.dr-chuck.com/dr-chuck/resume/bio.htm (3190) 3\n",
      "Row : (26, 'http://www.dr-chuck.com/dr-chuck/resume/pictures')\n",
      "26 http://www.dr-chuck.com/dr-chuck/resume/pictures (926) 0\n",
      "Row : None\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS Pages;\n",
    "DROP TABLE IF EXISTS Links;\n",
    "DROP TABLE IF EXISTS Webs;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        print(\"Row :\", row)\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567ba83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
