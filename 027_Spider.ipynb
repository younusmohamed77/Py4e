{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b576f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# import urllib.error\n",
    "# import ssl\n",
    "# from urllib.parse import urljoin\n",
    "# from urllib.parse import urlparse\n",
    "# from urllib.request import urlopen\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Ignore SSL certificate errors\n",
    "# ctx = ssl.create_default_context()\n",
    "# ctx.check_hostname = False\n",
    "# ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# conn = sqlite3.connect('spider.sqlite')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "#     (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "#      error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "# cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "#     (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))''')\n",
    "\n",
    "# cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# # Check to see if we are already in progress...\n",
    "# cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "# row = cur.fetchone()\n",
    "# if row is not None:\n",
    "#     print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "# else :\n",
    "#     starturl = input('Enter web url or enter: ')\n",
    "#     if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "#     if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "#     web = starturl\n",
    "#     if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "#         pos = starturl.rfind('/')\n",
    "#         web = starturl[:pos]\n",
    "\n",
    "#     if ( len(web) > 1 ) :\n",
    "#         cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "#         cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "#         conn.commit()\n",
    "\n",
    "# # Get the current webs\n",
    "# cur.execute('''SELECT url FROM Webs''')\n",
    "# webs = list()\n",
    "# for row in cur:\n",
    "#     webs.append(str(row[0]))\n",
    "\n",
    "# print(webs)\n",
    "\n",
    "# many = 0\n",
    "# while True:\n",
    "#     if ( many < 1 ) :\n",
    "#         sval = input('How many pages:')\n",
    "#         if ( len(sval) < 1 ) : break\n",
    "#         many = int(sval)\n",
    "#     many = many - 1\n",
    "\n",
    "#     cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "#     try:\n",
    "#         row = cur.fetchone()\n",
    "#         # print row\n",
    "#         fromid = row[0]\n",
    "#         url = row[1]\n",
    "#     except:\n",
    "#         print('No unretrieved HTML pages found')\n",
    "#         many = 0\n",
    "#         break\n",
    "\n",
    "#     print(fromid, url, end=' ')\n",
    "\n",
    "#     # If we are retrieving this page, there should be no links from it\n",
    "#     cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "#     try:\n",
    "#         document = urlopen(url, context=ctx)\n",
    "\n",
    "#         html = document.read()\n",
    "#         if document.getcode() != 200 :\n",
    "#             print(\"Error on page: \",document.getcode())\n",
    "#             cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "#         if 'text/html' != document.info().get_content_type() :\n",
    "#             print(\"Ignore non text/html page\")\n",
    "#             cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "#             conn.commit()\n",
    "#             continue\n",
    "\n",
    "#         print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "#         soup = BeautifulSoup(html, \"html.parser\")\n",
    "#     except KeyboardInterrupt:\n",
    "#         print('')\n",
    "#         print('Program interrupted by user...')\n",
    "#         break\n",
    "#     except:\n",
    "#         print(\"Unable to retrieve or parse page\")\n",
    "#         cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "#         conn.commit()\n",
    "#         continue\n",
    "\n",
    "#     cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "#     cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "#     conn.commit()\n",
    "\n",
    "#     # Retrieve all of the anchor tags\n",
    "#     tags = soup('a')\n",
    "#     count = 0\n",
    "#     for tag in tags:\n",
    "#         href = tag.get('href', None)\n",
    "#         if ( href is None ) : continue\n",
    "#         # Resolve relative references like href=\"/contact\"\n",
    "#         up = urlparse(href)\n",
    "#         if ( len(up.scheme) < 1 ) :\n",
    "#             href = urljoin(url, href)\n",
    "#         ipos = href.find('#')\n",
    "#         if ( ipos > 1 ) : href = href[:ipos]\n",
    "#         if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "#         if ( href.endswith('/') ) : href = href[:-1]\n",
    "#         # print href\n",
    "#         if ( len(href) < 1 ) : continue\n",
    "\n",
    "# \t\t# Check if the URL is in any of the webs\n",
    "#         found = False\n",
    "#         for web in webs:\n",
    "#             if ( href.startswith(web) ) :\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if not found : continue\n",
    "\n",
    "#         cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "#         count = count + 1\n",
    "#         conn.commit()\n",
    "\n",
    "#         cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "#         try:\n",
    "#             row = cur.fetchone()\n",
    "#             toid = row[0]\n",
    "#         except:\n",
    "#             print('Could not retrieve id')\n",
    "#             continue\n",
    "#         # print fromid, toid\n",
    "#         cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "#     print(count)\n",
    "\n",
    "# cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2567ba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: https://en.wikipedia.org/wiki/Main_Page\n",
      "['https://en.wikipedia.org/wiki/Main_Page']\n",
      "How many pages:10\n",
      "Row : (1, 'https://en.wikipedia.org/wiki/Main_Page')\n",
      "1 https://en.wikipedia.org/wiki/Main_Page (101767) 6\n",
      "Row : None\n",
      "Executed 1\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS Pages;\n",
    "DROP TABLE IF EXISTS Links;\n",
    "DROP TABLE IF EXISTS Webs''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( web, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        print(\"Row :\", row)\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print(\"Executed 1\")\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "#     print(\"Tags:\",tags)\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "#         print(\"Tag:\", tag)\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "#         print(\"UP:\", up)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "#         print(\"HREF:\", href)\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "        if many == 0:\n",
    "            print(\"Executed 2\")\n",
    "            print('No unretrieved HTML pages found')\n",
    "            many = 0\n",
    "            break\n",
    "            \n",
    "    print(count)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e408a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
